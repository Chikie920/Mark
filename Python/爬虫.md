**本节将包含少部分《Python编程快速上手》的内容，主要参考了崔庆才的《Python 3网络爬虫开发实战 》内容，同时还包含了实战的演练**



## 1 - 必要库的安装

`requests`库，用于从`Web`获取数据 `pip install requests`

`BeautifulSoup`库，解析网页源代码，`pip install beautifulsoup4`



## 2 - 爬虫基础

- `URL` 网页地址
- `HTTP`和`HTTPS`，这两个都是协议类型， `HTTPS`是`HTTP`的安全版，它传输的内容都是经过 `SSL`加密过的



### 2.1 - 请求

- 请求由客户端向服务端发出，可以分为4部分内容：请求方法、请求的网址、 请求头、 请求体。

- 常见的请求方法有 `GET和POST`

- 在浏览器中直接输入URL并回车， 这便发起了一个`GET` 请求



**`GET`和`POST`请求方法有如下区别**

- `GET`请求中的参数包含在`URL`里面，数据可以在`URL`中看到， 而`POST`请求的`URL`不会包
  含这些数据， 数据都是通过表单形式传输的， 会包含在请求体中。
- `GET`请求提交的数据最多只有`1024`字节，而`POST`方式没有限制。



**请求头**

请求头，用来说明服务器要使用的附加信息，比较重要的信息有`Cookie` 、`Referer` 、`User-Agent` 等

![image-20210705135109863](C:\Users\Mirai\AppData\Roaming\Typora\typora-user-images\image-20210705135109863.png)



### 2.2 - 响应

响应由服务端返回给客户端，可以分为三部分：响应状态码、响应头和响应体



## 3 - 基本库的使用



### 3.1 - 分析Robots协议

`Robots` 协议也称作爬虫协议、机器人协议，它的全名叫作网络爬虫排除标准，**用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取**。它通常是一个叫作`robots.txt`的文本文件，一般放在网站的根目录下。



**猫眼电影robots.txt示例**

```
User-agent: *
Disallow: /*?utm_source*
```

- `User-agent`为允许搜索的爬虫名称

如果设置 `User-agent: Baiduspider`，这表明网站只允许百度的爬虫爬取

这里设置 `*` 表明所有爬虫都允许

- `Disallow`指定不允许抓取的目录



### 3.2 - requests库的使用



**示例** - 使用`requests`模块的`get()`函数发送GET请求，获取网页源代码



```python
import requests

response = requests.get('http://httpbin.org/get')
print(response.text)

'''
{
  "args": {},
  "headers": {
    "Accept": "*/*",
    "Accept-Encoding": "gzip, deflate",
    "Host": "httpbin.org",
    "User-Agent": "python-requests/2.25.1",
    "X-Amzn-Trace-Id": "Root=1-60e2a335-304d0e5b1d5cb59b172ed401"
  },
  "origin": "117.150.215.29",
  "url": "http://httpbin.org/get"
}
'''
```



- `requests.get()`函数返回一个`Response`对象，该对象有一个 `.text`属性，储存了网页源代码，还有 `.json`属性，将源代码转为`Json`格式

- 还有一个 `status_code`属性，当网页请求成功时返回`requests.codes.ok`

- 检查成功的方法还有在`Response`对象上调用 `raise_for_status()` 方法，成功则继续，不成功会报错



#### 3.2.1 - 抓取网页



**示例：** 添加请求头抓取网页

如果没有浏览器信息，有些网页会禁止抓取



以猫眼电影的排行榜为例

```python
import requests

header = {
    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get('https://maoyan.com/board', headers=header)
print(response.text)

'''
<!DOCTYPE html>

<!--[if IE 8]><html class="ie8"><![endif]-->
<!--[if IE 9]><html class="ie9"><![endif]-->
<!--[if gt IE 9]><!--><html><!--<![endif]-->
<head>
  <title>热映口碑榜 - 猫眼电影 - 一网打尽好电影</title>
以下省略
'''
```

由于该页面是异步加载的，我们只能抓到前几个电影排行



#### 3.2.2 - 抓取二进制数据

`Reponse.content`返回的是`bytes`型也就是二进制的数据，**对于非文本内容我们需要使用该函数进行抓取**



**示例：**抓取电影封面并保存，我们的命令行不能输出图片，也没有相关函数支持，所以我们需要保存它

```python
import requests
from pathlib import Path

header = {
    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get('https://p1.meituan.net/movie/63cb3dbbaff624b236b2e22b2ee59cff1553093.jpg@160w_220h_1e_1c', headers=header)
img_path = open(Path().cwd() / '1921.jpg', 'wb')
img_path.write(response.content)
```



![image-20210705144540229](C:\Users\Mirai\AppData\Roaming\Typora\typora-user-images\image-20210705144540229.png)



#### 3.2.3 - Post请求

```python
import requests

data = {
    'cat' : 1,
    'dog' : 2,
    'cow' : 3
}
response = requests.post('http://httpbin.org/post', data=data)
print(response.status_code) #返回状态码
print(response.text)

'''
200
{
  "args": {},
  "data": "",
  "files": {},
  "form": {
    "cat": "1",
    "cow": "3",
    "dog": "2"
  },
  "headers": {
    "Accept": "*/*",
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "17",
    "Content-Type": "application/x-www-form-urlencoded",
    "Host": "httpbin.org",
    "User-Agent": "python-requests/2.25.1",
    "X-Amzn-Trace-Id": "Root=1-60e2ad56-014c9cca45dd2781329b88bd"
  },
  "json": null,
  "origin": "117.150.215.29",
  "url": "http://httpbin.org/post"
}
'''
```



#### 3.2.4 - 高级用法



**文件上传**

```python
import requests

file = {'file':open('1921.jpg', 'rb')}
response = requests.post('http://httpbin.org/post', files=file)
print(response.status_code)
print(response.text)

'''
200
{
  "args": {},
  "data": "",
  "files": {
    "file": "data:application/octet-stream;base64,/9j/4AA...图像的二进制码
'''
```



**Cookies**

可以复制网页的`Cookies`并在`headers`内添加键-值就行了

```python
header = {
    'cookies' : 'xxxxxx'
}
```



### 3.3 - 正则表达式

这个前面有讲过，不再赘述

![image-20210705150615291](C:\Users\Mirai\AppData\Roaming\Typora\typora-user-images\image-20210705150615291.png)



### 3.4 - 抓取猫眼电影Top100排行榜



**1.分析每页URL**

遗憾的是，排行榜并不是都放在一页的，我们需要分析每页`URL`是否有相同点，以便后续构造`URL`获取网页数据，因为python并不会自动帮我们翻网页



`URL_One` ：`https://maoyan.com/board/4?offset=0`

`URL_Two` ：`https://maoyan.com/board/4?offset=10`

`URL_Three` ：`https://maoyan.com/board/4?offset=20`

`URL_Ten` ：`https://maoyan.com/board/4?offset=90`

找到规律了



```python
import requests, re, time

header = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    #建议加Cookie
}

url = 'https://maoyan.com/board/4?offset='
response = requests.get('https://maoyan.com/board/4?offset='+'0', headers=header)

try:
    if response.status_code != 200:
        raise '网页请求失败!'
except Exception as error:
    print(error)

def web_response():
    target_url = 0
    for i in range(1, 10):
        target_url+=i*10
        response = requests.get(url+str(target_url), headers=header)
        parse_web(response.text)
        time.sleep(2)


def parse_web(res):
    regex = re.compile('<dd>.*?board-index.*?>(.*?)</i>.*?star.*?>(.*?)</p>.*?releasetime.*?>(.*?)</p>', flags=re.DOTALL)
    result = re.findall(regex, res)
    print(result)

parse_web(response.text)
web_response()
```

额，由于测试次数过多，一直被反爬，只测试出了一次结果，不过输出样式不是很好看这里就不放了



## 4 - 解析库的使用

只讲`BeautifulSoup`解析库

使用解析库比使用正则表达式更方便且高效



**解析器**

![image-20210705184509251](C:\Users\Mirai\AppData\Roaming\Typora\typora-user-images\image-20210705184509251.png)



```python
import bs4

target = '''
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title" name="dromouse"><b>The Dormouse's story</b></p>
<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1"><!-- Elsie --></a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>
<p class="story">...</p>
'''
soup = bs4.BeautifulSoup(target, 'html.parser')
print(soup.prettify())
print(soup.title.string)

'''
<html>
 <head>
  <title>
   The Dormouse's story
  </title>
 </head>
 <body>
  <p class="title" name="dromouse">
   <b>
    The Dormouse's story
   </b>
  </p>
  <p class="story">
   Once upon a time there were three little sisters; and their names were
   <a class="sister" href="http://example.com/elsie" id="link1">
    <!-- Elsie -->
   </a>
   ,
   <a class="sister" href="http://example.com/lacie" id="link2">
    Lacie
   </a>
   and
   <a class="sister" href="http://example.com/tillie" id="link3">
    Tillie
   </a>
   ;
and they lived at the bottom of a well.
  </p>
  <p class="story">
   ...
  </p>
 </body>
</html>
The Dormouse's story
'''
```

- 调用`prettify()`方法，这个方法可以把要解析的字符串以标准的缩进格式输出

- 对于不标准的HTML 字符串`BeautifolSoup` ，可以自动更正格式



**结点选择**

又叫`Tag`选择， `bs对象.tag`

调用`soup.title.string` ，这实际上是输出HTML 中`title` 节点的文本内容。所以， `soup.title`可以选出HTML中的`title` 节点，再调用`string` 属性就可以得到里面的文本了



- 当有多个节点时，这种选择方式只会选择到第一个匹配的节点，其他的后面节点都会忽略



获取 `Tag`属性

如`id`或是`class`，可调用`attrs` 获取

```python
print(soup.p.attrs)
print(soup.p.attrs['name'])
'''
{'class': ['title'], 'name': 'dromouse'}
dromouse
'''
```

调用`string`属性获取内容



**嵌套选择**

```python
print(soup.p.b.string)
'''
The Dormouse's story
'''
```



**方法选择器**

